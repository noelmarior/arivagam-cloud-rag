1. Define Artificial Intelligence (AI)
1. AI as Computational Intelligence: Artificial Intelligence (AI) is generally perceived as computational intelligence that emulates the human mind, although this specific characterization does not hold true for all types of AI systems.
2. Intelligent Agents: In computer science, AI research is formally defined as the study of “intelligent agents,” which are any devices that perceive their environment and take actions designed to maximize their chances of successfully achieving their defined goals.
3. Mimicking Cognitive Functions: Colloquially, the term AI is applied when a machine successfully mimics the "cognitive" functions that humans associate with other human minds, such as "learning" and "problem-solving".
4. Task Execution: AI enables computers to execute tasks that were historically performed by human intelligence, encompassing fields like machine learning, robotics, Natural Language Processing (NLP), synthetic intelligence, and text mining.
2. What are the functions and applications of Artificial Intelligence?
1. Core Functions: The functions of artificial intelligence include Machine Learning, Natural Language Processing (NLP), Immersive Experiences (such as Virtual Reality and Augmented Reality), and Robotics.
2. Healthcare and Diagnosis: In healthcare, AI is becoming highly advantageous for making better and faster diagnoses than humans, and it can inform doctors when a patient’s condition is worsening.
3. Finance Industry: The finance industry implements AI through automation, chatbots, adaptive intelligence, algorithm trading, and machine learning into various financial processes.
4. Gaming and Strategy: AI can be used for playing strategic games like chess, where the machine must consider a large number of possible moves.
3. Define Cell and Chromosome.
1. Neuron/Nerve Cell (Biological Context): Neurons (also known as Nerve Cells) are the fundamental units of the brain and nervous system, responsible for receiving input from the external world and sending output (commands to muscles).
2. Neuron Functionality: These cells transform electrical signals in between input and output. The simplest model of a neuron is a processing element (PE) that produces an output if the sum of the inputs exceeds an internal threshold value.
3. Chromosome (Genetic Algorithm Context): In the context of Genetic Algorithms (GA), a chromosome indicates a possible solution to a given problem. The collection of these solutions constitutes the population.
4. Gene and Allele (GA Context): A chromosome consists of gene elements; a gene is one element position of a chromosome, and an allele is the value that a gene takes for a particular chromosome.
4. What are the advantages of Bayesian network in Artificial Intelligence?
1. Efficient Joint Probability: Bayesian networks provide a more efficient way to represent and compute joint probabilities by effectively utilizing the conditional independence properties inherent in the variables of the network.
2. Extensibility: They are highly extensible, meaning that adding a new piece of information to the network requires only a few probabilities and a few new edges in the graph.
3. Compact Representation: A Bayesian network offers a compact, flexible, and interpretable representation of a joint probability distribution.
4. Causal Relationships: They are useful in knowledge discovery because their directed acyclic graphs allow for the representation of causal relations between different variables.
5. Write about ‘Overfitting’ in Machine Learning.
1. Definition: Overfitting occurs when a machine learning model attempts to cover all or more than the required data points present in the training dataset.
2. Learning Noise: Due to overfitting, the model begins to catch noise and irrelevant, inaccurate values present in the dataset.
3. Impact and Characteristics: An overfitted model performs poorly on unseen data because it learns noise rather than the underlying pattern, leading to high variance and low bias.
4. Mitigation Techniques: Ways to avoid overfitting include using Cross-Validation techniques, training the model with more data, applying Regularization (like Ridge or Lasso), early stopping the training process, and using Ensemble methods.
6. Do you think 50 small decision trees are better than a large one? why?
1. Ensemble Approach: Yes, generally 50 small decision trees are better because this configuration mirrors the Random Forest algorithm, an ensemble learning method that combines multiple decision trees to improve overall predictive accuracy.
2. Prediction Mechanism: Instead of relying on a single, potentially complex decision tree, Random Forest takes predictions from each small tree and bases the final output on the majority votes (for classification) or the average (for regression).
3. Preventing Overfitting: Using a large number of trees (a forest) significantly prevents the problem of overfitting, which is a major drawback of using a single large decision tree.
4. Enhanced Accuracy: This ensemble approach enhances the accuracy of the model, is capable of efficiently handling large datasets with high dimensionality, and can maintain accuracy even when a large proportion of data is missing.
7. What are the types of activation functions?
1. Primary Categories: Activation functions are basically divided into two types: Linear Activation Functions and Non-linear Activation Functions.
2. Linear Function: The Linear Activation Function is represented by the equation f(x)=x, and its range spans from negative infinity to infinity, but it fails to address the complexity of typical neural network data.
3. Common Non-linear Functions: The most commonly used activation functions are Non-linear, which include the Sigmoid (or Logistic) Activation Function (S-shaped curve) and the Tanh (or hyperbolic tangent) Activation Function.
4. Purpose of Non-linearity: The use of non-linear functions helps the model to generalize and adapt to a variety of data, making it easier to differentiate between outputs and learn complex relationships.
8. Identify Which of the following is a supervised learning problem?
• The correct options that represent supervised learning problems are B and C.
1. Predicting Credit Approval (B): This is a supervised learning problem because predicting a discrete or categorical outcome (approval or denial) falls under Classification, which is a supervised technique.
2. Predicting Rainfall (C): This is also a supervised learning problem because predicting a continuous/real value (the amount of rainfall) falls under Regression, a supervised technique used for prediction and forecasting.
3. Supervised Learning Requirement: Supervised learning requires the algorithm to be trained on input data that has been labeled with the corresponding desired output, allowing the model to detect underlying patterns.
4. Grouping People (A): This task, known as clustering, is a core method of Unsupervised Learning, which is used to uncover relationships and insights in unlabeled datasets rather than predicting a label based on training data.
9. Compare Precision and Recall.
1. Precision Definition: Precision measures the proportion of positive predictions made by the model that were actually correct in reality.
2. Precision Calculation Focus: It is calculated by dividing the True Positives (correctly predicted true outcomes) by the total positive predictions (True Positives + False Positives, FP). Maximizing precision minimizes False Positive errors.
3. Recall Definition: Recall (also known as Sensitivity) aims to calculate the proportion of all actual positive cases that were correctly identified by the model.
4. Recall Calculation Focus: It is calculated by dividing the True Positives by the total number of actual positive cases (True Positives + False Negatives, FN). Maximizing recall minimizes False Negative errors.
10. Mention the features of SVM regression.
1. Algorithm Type: Support Vector Regression (SVR) is a regression algorithm based on the Support Vector Machine (SVM) supervised learning technique, which specifically works for continuous variables.
2. Hyperplane Purpose: In SVR, the hyperplane is defined as a line that helps predict continuous variables and is designed to cover the maximum number of data points.
3. Boundary Lines: Boundary lines, which run parallel to the hyperplane, create a margin for data points, and SVR seeks to determine a hyperplane with the maximum margin.
4. Kernel Function: SVR employs Kernel functions, which map lower-dimensional data into higher-dimensional data spaces.
11. Mention the components of expert system.
1. Knowledge Base: This acts as storage, containing knowledge acquired from various domain experts, including factual and heuristic knowledge, often formalized using If-else rules.
2. Inference Engine: Considered the "brain" of the expert system, it is the main processing unit that applies inference rules to the knowledge base to derive conclusions or deduce new information, helping to find error-free solutions to user queries.
3. User Interface: This component enables a non-expert user to communicate with the expert system, receiving queries as input and displaying the output from the inference engine in a readable format.
4. System Description: Expert systems are a type of computer program designed to solve complex problems and provide decision-making ability akin to a human expert, basing their performance on the knowledge stored in the knowledge base.
12. What is meant by Knowledge base system.
1. Definition: A knowledge-based system (KBS) is a computer system that utilizes AI concepts to analyze knowledge, data, and information from various sources to generate new knowledge.
2. Problem Solving: These systems are often equipped with built-in problem-solving capabilities, allowing them to understand the context of the data they review and store in order to make informed decisions.
3. Core Structure: The knowledge-based system typically consists of three components: the Knowledge Base (the repository of information), the Interface Engine (processes data to locate relevant information), and the User Interface (the presentation layer for user interaction).
4. Use Cases: KBS is useful for providing expertise, making expert decisions, creating new knowledge by reviewing existing data, and handling significant amounts of structured and unstructured data intelligently and efficiently.
13. Mention most common gaming techniques used in Artificial Intelligence?
1. AI in Strategy Games: AI is specifically used in game playing for strategic games, such as chess, where the machine must consider a massive number of possible moves.
2. Reinforcement Learning (RL): RL is employed in game playing applications (e.g., tic-tac-toe and chess) to find the best possible behavior or path an agent should take to maximize a reward.
3. Genetic Algorithms (GA): GA, a search-based optimization technique, is applied in game designing to solve optimization problems.
4. Multi-Armed Bandit Problem (MABP): MABP algorithms can be used in game designing to test experimental changes in game play/interface and then exploit the changes that result in positive experiences for players.
14. Is genetic algorithm an AI algorithm? Write two main features of GA
1. AI Context: Genetic Algorithm (GA) is a search-based optimization technique. It is a form of Evolutionary Computation, which is one of the methods used to implement Computational Intelligence, a branch of Artificial Intelligence.
2. Feature: Optimization: GA is frequently used to find optimal or near-optimal solutions to difficult problems that require maximization or minimization of one or more objective functions.
3. Feature: Chromosomes/Solutions: The algorithm is based on the genetic structure of the population's chromosomes, where each chromosome represents a possible solution to the given problem.
4. Feature: Fitness Function: A fitness function evaluates how close a given candidate solution is to the optimum solution of the desired problem, determining how "fit" the solution is.
15. List any two real time applications of Artificial Intelligence in industry.
1. Autonomous Vehicles: AI, via deep learning technology, is a key component behind driverless cars, enabling them to recognize objects like stop signs or distinguish pedestrians from lampposts.
2. E-commerce Recommendations: Companies like Netflix and Amazon use ML/AI algorithms to analyze vast amounts of data regarding user interest and provide recommendations for products or programs.
3. Finance and Fraud Detection: AI is implemented in the finance industry to detect potential fraud and suspicious activity.
4. Cybersecurity: AI is used to enhance data safety and security, for instance, by determining software bugs and cyber-attacks using tools like AEG bot and AI2 Platform.
16. What is ‘Training Set’ and ‘Test Set’?
1. Training Set Role: The training set consists of sample historical data that is provided to machine learning algorithms to help them build a mathematical model.
2. Training Goal: Training a model is required so that it can understand the various underlying patterns, rules, and features necessary to predict an outcome.
3. Test Set Role: Once the machine learning model has been trained on the given dataset, the test set is used to check the model's accuracy.
4. Generalization Check: Testing the model determines the percentage accuracy of the model. A model aims to generalize well, meaning it performs reliably on this unseen test data.
17. Define Activation Function in neural network.
1. Activation Decision: The activation function calculates the weighted sum and adds a bias to it, subsequently deciding whether a neuron should be activated or not.
2. Non-linearity Introduction: The purpose of the activation function is critical: it introduces non-linearity into the output of a neuron, which allows the network to model more complex relationships.
3. Backpropagation Support: Activation functions are essential because they make the backpropagation process possible by supplying the gradients needed to update the weights and biases based on the calculated error.
4. Output Mapping: Activation functions map the resulting values into a predefined range, such as between 0 and 1 or -1 and 1, depending on the specific function chosen.
18. What is MLP and how does it work?
1. Definition and Structure: Multi-layer Perceptron (MLP) is a type of neural network that has multiple layers, also known as fully connected dense layers. It includes one input layer, one output layer, and one or more hidden layers.
2. Working Mechanism (Feedforward): MLP falls under the category of feedforward algorithms. Inputs are combined with initial weights in a weighted sum, subjected to an activation function (like sigmoid), and then propagated to the next layer.
3. Layer Propagation: Each layer computes its internal representation of the data and feeds that result to the subsequent layer, continuing all the way through the hidden layers to the output layer.
4. Learning (Backpropagation): The learning mechanism in MLP is Backpropagation, which allows the system to iteratively adjust the weights in the network with the goal of minimizing the calculated cost function.
19. Compute the accuracy metrics in confusion matrix.
1. Precision Metric: Precision determines the proportion of positive predictions that were actually correct. The calculation involves dividing True Positive (TP) predictions by the total positive predictions (TP + False Positive, FP).
2. Recall (Sensitivity) Metric: Recall calculates the proportion of all actual positive cases that were correctly identified. The calculation involves dividing True Positive (TP) predictions by the total number of actual positives (TP + False Negative, FN).
3. Confusion Matrix Components: The matrix is divided into four terminologies used for these calculations: True Positive (TP), True Negative (TN), False Positive (FP), and False Negative (FN).
4. Importance: Metrics derived from the confusion matrix, like Precision and Recall, help evaluate a model’s performance, particularly when the target variable classes are imbalanced, making simple accuracy an unreliable measure.
20. What is Machine Learning and mention its applications.
1. Definition: Machine Learning (ML) is a subset of Artificial Intelligence that focuses on developing algorithms that enable computers to learn automatically from past data and experiences without being explicitly programmed.
2. Model Building: ML uses various algorithms to build mathematical models and make predictions using historical data or information.
3. Applications in Recognition: ML is currently used for tasks such as image recognition, speech recognition, and Facebook auto-tagging.
4. Applications in Prediction: Modern ML models are employed for predictions including weather prediction, disease prediction, stock market analysis, and developing recommender systems.
21. What are the characteristics of Artificial Intelligence?
1. Automation of Monotonous Tasks: AI systems are capable of continually executing monotonous tasks as instructed, which helps in minimizing both human errors and costs.
2. Handling Big Data: AI systems are capable of managing and analyzing large volumes of data (data ingestion) gathered from multiple sources.
3. Mimicking Cognition: A defining characteristic is the imitation of human cognition, where these systems mimic the human mind's process of thinking, interpreting the environment, and solving problems to make decisions.
4. Futuristic and Planning: Businesses using AI can rely on environmental sensing to find opportunities, and technologies like machine learning can be fed into algorithms to obtain a certain target in various scenarios.
22. How is Machine Learning used in real life applications?
1. Healthcare and Medicine: ML models are used for medical diagnosis, aiding in the identification of diseases. They can also be used to discover genetic sequences linked to diseases.
2. Autonomous Navigation: ML is a core technology used in self-driving cars and for creating auto-piloting planes.
3. E-mail and Spam Filtering: ML algorithms are effectively used to identify and filter unwanted spam messages from emails.
4. Customer Segmentation and Marketing: ML is used for segmentation of customer behavior and for developing targeted advertising strategies for specific consumers.
23. Define State Space Search Technique.
1. AI Context: State Space Search is a fundamental problem-solving approach in Artificial Intelligence used to explore all possible states of a system to reach a desired goal state from an initial state.
2. Feature: Representation: The entire problem is represented as a set of states, actions, and transitions forming a search tree or graph.
3. Feature: Exploration: It systematically explores possible states using strategies like Breadth-First, Depth-First, or Heuristic Search.
4. Feature: Goal Test: A goal test function determines whether the current state satisfies the desired objective, marking the end of the search process.
24. Mention the criteria for the evaluation of search strategy.
1. AI Context: Search strategies in AI are evaluated to determine their efficiency, effectiveness, and practicality in reaching the goal state from the initial state.
2. Criterion: Completeness: A strategy is complete if it guarantees finding a solution whenever one exists.
3. Criterion: Optimality: A search method is optimal if it always finds the best (least-cost or shortest) solution among possible ones.
4. Criterion: Time & Space Complexity: These measure how much time and memory the algorithm consumes, indicating its computational feasibility.
25. What are the five popular algorithms of Machine Learning?
1. Classification Algorithms: Algorithms commonly used for predicting discrete values include Logistic Regression and Support Vector Machines (SVM).
2. Tree-Based Algorithms: Decision Tree Classification is popular, especially as a base model for ensemble methods.
3. Ensemble Method: Random Forest Classification is considered one of the most powerful supervised learning algorithms, capable of regression and classification.
4. Lazy/Non-Parametric Algorithms: K-Nearest Neighbors (KNN) is a non-parametric, lazy learner algorithm primarily used for classification based on similarity measures.
26. Define Association in unsupervised learning.
1. Definition: Association is an unsupervised learning method used for uncovering and finding interesting relationships or patterns between variables in a large database or transactional data.
2. Mechanism: It determines the set of items that occur together within a dataset, thus checking for the dependency of one data item on another.
3. Rule Generation: It is based on different rules (e.g., if A then B) to discover these relations, where 'If' is called the antecedent and 'Then' is the consequent.
4. Application Example: A key application is Market Basket Analysis, where retailers analyze purchase data to identify relationships, for example, noting that customers who buy bread tend to buy jam.
27. Why ReLu used in deep learning?
1. Activation Function Role: ReLU (Rectified Linear Unit) is one of the activation functions used in the hidden layers of neural networks.
2. Non-linearity: Its primary role is to introduce non-linearity into the network, which is necessary for the neural network to learn and model complex relationships between the inputs and outputs.
3. Training Speed: ReLU is often used in deep learning because it tends to work well and contributes to improved training speed compared to other functions.
4. Hyperparameter Choice: Although ReLU is commonly used, the choice of activation function depends on the specific problem being addressed.
28. Define Perceptron.
1. Basic Neuron Unit: A perceptron is the term used to describe a basic neuron or unit within a neural network.
2. Origin of Neural Networks: Neural Networks are essentially multi-layer Perceptrons (MLP), and the perceptron model defines the introductory step into multi-layered neural networks.
3. Threshold Requirement: In the Perceptron model, the neuron must possess an activation function (like ReLU or sigmoid) that imposes a fixed threshold on the output.
4. Functionality: A perceptron combines inputs with weights in a weighted sum, subjects the result to the activation function, and outputs the result.
29. State about Supervised Learning.
1. Labeled Data Training: In supervised learning, the algorithm is trained on input data that has been labeled for a particular desired output.
2. Pattern Detection: The model trains until it successfully detects the underlying patterns and relationships that exist between the provided input data and the corresponding output labels.
3. Generalization: This allows the model to subsequently yield accurate labeling or prediction results when presented with never-before-seen data.
4. Problem Classification: Supervised learning problems are typically divided into two main categories: Classification (predicting discrete values) and Regression (predicting continuous or real values).
30. How K-Means algorithm is used in Unsupervised Learning.
1. Unsupervised Context: K-Means Clustering is an Unsupervised Learning algorithm used to solve clustering problems by grouping the unlabeled dataset into distinct clusters.
2. Centroid-Based: It is a centroid-based iterative algorithm where each cluster is associated with a centroid, and the algorithm requires the value of K (the number of clusters) to be predetermined.
3. Objective: The primary goal of the algorithm is to minimize the sum of distances between each individual data point and the centroid of its assigned cluster.
4. Clustering Process: K-Means performs two main tasks: determining the optimal location for the K centroids and then assigning each data point to its closest K-center, ensuring that each cluster contains data points with shared commonalities.